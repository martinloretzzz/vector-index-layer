{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/martinloretzzz/vector-index-layer/blob/main/GPTVectorIndexOnOutEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsNCstCKwMqG",
    "outputId": "56bbe1bf-7b52-40a0-fce2-eb16f0a5ade3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hnswlib in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hnswlib) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m9Fv-bjlx8Ot"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Model\n",
    "import hnswlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a60r2gddzutA",
    "outputId": "04c03dd0-0338-4280-bc7c-4d0b67c59e24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox-like creatures, known as vortigaunts called Vortigaunts, were introduced to the United States in the late 1940s.\n",
      "\n",
      "They are a type of fox that can grow large in size and is capable of\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model_headless = GPT2Model.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    return_dict_in_generate=True,\n",
    "    output_hidden_states=True,\n",
    "    output_scores=True,\n",
    "    output_logits=True\n",
    ")\n",
    "\n",
    "gen_tokens = output.sequences\n",
    "\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lh9EoUL5Dul",
    "outputId": "4517ffd4-49d9-4649-f525-6c6d223f684d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46 13\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "hidden = output.hidden_states\n",
    "logits = output.logits\n",
    "\n",
    "last_hidden = hidden[-1][-1].squeeze(0)\n",
    "last_logits = logits[-1]\n",
    "\n",
    "print(len(hidden), len(logits), len(hidden[-1]))\n",
    "print(last_hidden.shape)\n",
    "print(last_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F5xvNoOPxPmJ"
   },
   "outputs": [],
   "source": [
    "class HNSWIndexEmbedding():\n",
    "    def __init__(self, weight, k, M=32, ef=100, ef_construction=100):\n",
    "        self.k, self.vocab_size, self.dim = k, weight.shape[0], weight.shape[1]\n",
    "        self.index = hnswlib.Index(space='ip', dim=self.dim)\n",
    "        self.index.init_index(max_elements=self.vocab_size, M=M, ef_construction=ef_construction, random_seed=42)\n",
    "        self.index.add_items(weight.numpy())\n",
    "        self.index.set_ef(ef)\n",
    "\n",
    "    def forward(self, x):\n",
    "        indices, distances = self.index.knn_query(x.detach().cpu().numpy(), k=self.k)\n",
    "        return torch.from_numpy(1 - distances).to(torch.float32).to(x.device), torch.from_numpy(indices).to(torch.int64).to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYOubClf4eBb",
    "outputId": "e6b3507b-6e9b-4085-ca11-686416035c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "\n",
    "out_emb_weight = model.transformer.wte.weight.detach().clone()\n",
    "print(out_emb_weight.shape)\n",
    "\n",
    "out_emb_vector = HNSWIndexEmbedding(out_emb_weight, k=k, ef_construction=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrjPOoSq5xJk",
    "outputId": "3ca6507c-5d9e-48cd-d4db-f95a53ce5bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-226.4115, -233.0608, -233.6529, -233.8399, -234.1039, -234.7538,\n",
       "          -235.2826, -235.3414, -235.3539, -235.3834, -235.5113, -235.6125,\n",
       "          -235.6129, -235.7076, -235.7217, -235.7992, -235.9429, -236.0075,\n",
       "          -236.0421, -236.0859, -236.2032, -236.2416, -236.2963, -236.3992,\n",
       "          -236.4150, -236.5225, -236.7359, -236.7570, -236.7627, -236.7924,\n",
       "          -236.8477, -236.9031, -236.9309, -237.2243, -237.3063, -237.3725,\n",
       "          -237.3989, -237.5249, -237.5824, -237.5862, -237.6090, -237.6250,\n",
       "          -237.6516, -237.6921, -237.7494, -237.7602, -237.7992, -237.8032,\n",
       "          -237.8066, -237.8293]]),\n",
       " tensor([[ 286,  284,   11,  329,  287,  290,  379,  351,  772,  355,  510,  326,\n",
       "           319,  357, 1111,  422,  198,   13,  503,  257,  262,  517, 2048,  366,\n",
       "           618,  416,  407,  691,  393,  611,  532,  625,  460,  475,  572, 2035,\n",
       "           546, 1576,  481,  655, 2592,   12, 3016,  256,  477,  523,  739,   25,\n",
       "           428,  706]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_logits, out_indices = out_emb_vector.forward(last_hidden)\n",
    "out_logits, out_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oufU6Ml-B95W"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    data = torch.cat([hidden[i][-1].squeeze(0) for i in range(len(hidden))], dim=0).repeat(6, 1)\n",
    "    time_repeat, time_num = 10, 10\n",
    "\n",
    "    print(\"| B   | ef  | Speedup |\")\n",
    "    print(\"| --: | --: | ------: |\")\n",
    "    for ef in [100, 200]:\n",
    "      for B in [1, 8, 64, 256]:\n",
    "        out_emb_vector.index.set_ef(ef)\n",
    "        batch = data[0:B, :]\n",
    "\n",
    "        forward_time = min(timeit.repeat(lambda: out_emb_vector.forward(batch), number=time_num, repeat=time_repeat))\n",
    "        forward_ref_time = min(timeit.repeat(lambda: batch @ out_emb_weight.T, number=time_num, repeat=time_repeat))\n",
    "\n",
    "        print(f\"|  {B}  | {ef} | {forward_ref_time / forward_time:.1f}x |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MubsgL5kLWsO",
    "outputId": "ef1c2f15-efb7-4948-ea9d-bebe322b5bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time taken (forward): 0.012672 seconds\n",
      "Average time taken (matrix multiplication): 0.199741 seconds\n",
      "Speedup: 15.7622\n"
     ]
    }
   ],
   "source": [
    "# TODO measure on the GPU\n",
    "\n",
    "out_emb_vector.index.set_ef(100)\n",
    "\n",
    "forward_time = timeit.timeit(lambda: out_emb_vector.forward(last_hidden), number=10)\n",
    "forward_ref_time = timeit.timeit(lambda: last_hidden @ out_emb_weight.T, number=10)\n",
    "\n",
    "print(f\"Average time taken (forward): {forward_time:.6f} seconds\")\n",
    "print(f\"Average time taken (matrix multiplication): {forward_ref_time:.6f} seconds\")\n",
    "print(f\"Speedup: {forward_ref_time / forward_time:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECv9TITu701E",
    "outputId": "509a9c33-9541-4c47-ab27-3ade55fd6865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m0: 6/10 (0.60), logits: 0.5174, es 274\u001b[0m\n",
      "\u001b[33m0: 15/50 (0.30), logits: 0.4504, es 274\u001b[0m\n",
      "\n",
      "1: 10/10 (1.00), logits: 1.0000, like 2339\u001b[0m\n",
      "1: 48/50 (0.96), logits: 0.9810, like 2339\u001b[0m\n",
      "\n",
      "2: 10/10 (1.00), logits: 1.0000,  creature 7185\u001b[0m\n",
      "2: 44/50 (0.88), logits: 0.9229,  creature 7185\u001b[0m\n",
      "\n",
      "3: 10/10 (1.00), logits: 1.0000,  are 389\u001b[0m\n",
      "3: 50/50 (1.00), logits: 1.0000,  are 389\u001b[0m\n",
      "\n",
      "4: 10/10 (1.00), logits: 1.0000,  which 543\u001b[0m\n",
      "4: 48/50 (0.96), logits: 0.9864,  which 543\u001b[0m\n",
      "\n",
      "5: 9/10 (0.90), logits: 0.9880,  as 355\u001b[0m\n",
      "5: 44/50 (0.88), logits: 0.9782,  as 355\u001b[0m\n",
      "\n",
      "6: 9/10 (0.90), logits: 0.9263,  the 262\u001b[0m\n",
      "6: 44/50 (0.88), logits: 0.9226,  the 262\u001b[0m\n",
      "\n",
      "7: 10/10 (1.00), logits: 1.0000, ult 586\u001b[0m\n",
      "7: 48/50 (0.96), logits: 0.9779, ult 586\u001b[0m\n",
      "\n",
      "8: 10/10 (1.00), logits: 1.0000, ices 1063\u001b[0m\n",
      "8: 47/50 (0.94), logits: 0.9924, ices 1063\u001b[0m\n",
      "\n",
      "\u001b[33m9: 8/10 (0.80), logits: 0.1246, aunts 43981\u001b[0m\n",
      "\u001b[33m9: 45/50 (0.90), logits: 0.1655, aunts 43981\u001b[0m\n",
      "\n",
      "10: 10/10 (1.00), logits: 1.0000, , 11\u001b[0m\n",
      "10: 50/50 (1.00), logits: 1.0000, , 11\u001b[0m\n",
      "\n",
      "11: 9/10 (0.90), logits: 0.9339,  \" 366\u001b[0m\n",
      "11: 42/50 (0.84), logits: 0.9107,  \" 366\u001b[0m\n",
      "\n",
      "12: 9/10 (0.90), logits: 0.9612, ort 419\u001b[0m\n",
      "12: 46/50 (0.92), logits: 0.9461, ort 419\u001b[0m\n",
      "\n",
      "13: 10/10 (1.00), logits: 1.0000, ig 328\u001b[0m\n",
      "13: 49/50 (0.98), logits: 0.9970, ig 328\u001b[0m\n",
      "\n",
      "14: 10/10 (1.00), logits: 1.0000, aunts 43981\u001b[0m\n",
      "14: 46/50 (0.92), logits: 0.9920, aunts 43981\u001b[0m\n",
      "\n",
      "15: 10/10 (1.00), logits: 1.0000, , 11\u001b[0m\n",
      "15: 49/50 (0.98), logits: 0.9988, , 11\u001b[0m\n",
      "\n",
      "16: 9/10 (0.90), logits: 0.9827,  are 389\u001b[0m\n",
      "16: 41/50 (0.82), logits: 0.9408,  are 389\u001b[0m\n",
      "\n",
      "17: 10/10 (1.00), logits: 1.0000,  a 257\u001b[0m\n",
      "17: 47/50 (0.94), logits: 0.9664,  a 257\u001b[0m\n",
      "\n",
      "18: 10/10 (1.00), logits: 1.0000,  to 284\u001b[0m\n",
      "18: 50/50 (1.00), logits: 1.0000,  to 284\u001b[0m\n",
      "\n",
      "19: 10/10 (1.00), logits: 1.0000,  the 262\u001b[0m\n",
      "19: 47/50 (0.94), logits: 0.9886,  the 262\u001b[0m\n",
      "\n",
      "20: 9/10 (0.90), logits: 0.9505,  world 995\u001b[0m\n",
      "20: 42/50 (0.84), logits: 0.9068,  world 995\u001b[0m\n",
      "\n",
      "21: 10/10 (1.00), logits: 1.0000,  States 1829\u001b[0m\n",
      "21: 41/50 (0.82), logits: 0.9997,  States 1829\u001b[0m\n",
      "\n",
      "22: 10/10 (1.00), logits: 1.0000,  in 287\u001b[0m\n",
      "22: 50/50 (1.00), logits: 1.0000,  in 287\u001b[0m\n",
      "\n",
      "\u001b[33m23: 6/10 (0.60), logits: 0.0478,  the 262\u001b[0m\n",
      "\u001b[33m23: 43/50 (0.86), logits: 0.2065,  the 262\u001b[0m\n",
      "\n",
      "24: 10/10 (1.00), logits: 1.0000,  late 2739\u001b[0m\n",
      "24: 49/50 (0.98), logits: 0.9978,  late 2739\u001b[0m\n",
      "\n",
      "25: 10/10 (1.00), logits: 1.0000,  1800 21431\u001b[0m\n",
      "25: 49/50 (0.98), logits: 0.9957,  1800 21431\u001b[0m\n",
      "\n",
      "26: 10/10 (1.00), logits: 1.0000, s 82\u001b[0m\n",
      "26: 46/50 (0.92), logits: 1.0000, s 82\u001b[0m\n",
      "\n",
      "27: 10/10 (1.00), logits: 1.0000, . 13\u001b[0m\n",
      "27: 50/50 (1.00), logits: 1.0000, . 13\u001b[0m\n",
      "\n",
      "28: 9/10 (0.90), logits: 0.9492,  They 1119\u001b[0m\n",
      "28: 48/50 (0.96), logits: 0.9583,  They 1119\u001b[0m\n",
      "\n",
      "29: 10/10 (1.00), logits: 1.0000, \n",
      " 198\u001b[0m\n",
      "29: 50/50 (1.00), logits: 1.0000, \n",
      " 198\u001b[0m\n",
      "\n",
      "30: 10/10 (1.00), logits: 1.0000, The 464\u001b[0m\n",
      "30: 48/50 (0.96), logits: 0.9837, The 464\u001b[0m\n",
      "\n",
      "31: 10/10 (1.00), logits: 1.0000,  were 547\u001b[0m\n",
      "31: 46/50 (0.92), logits: 0.9853,  were 547\u001b[0m\n",
      "\n",
      "32: 10/10 (1.00), logits: 1.0000,  a 257\u001b[0m\n",
      "32: 47/50 (0.94), logits: 0.9542,  a 257\u001b[0m\n",
      "\n",
      "33: 9/10 (0.90), logits: 0.9083,  large 1588\u001b[0m\n",
      "\u001b[33m33: 42/50 (0.84), logits: 0.8547,  large 1588\u001b[0m\n",
      "\n",
      "34: 10/10 (1.00), logits: 1.0000,  of 286\u001b[0m\n",
      "34: 46/50 (0.92), logits: 0.9998,  of 286\u001b[0m\n",
      "\n",
      "35: 10/10 (1.00), logits: 1.0000,  fox 21831\u001b[0m\n",
      "35: 40/50 (0.80), logits: 0.9005,  fox 21831\u001b[0m\n",
      "\n",
      "36: 10/10 (1.00), logits: 1.0000,  that 326\u001b[0m\n",
      "36: 36/50 (0.72), logits: 0.9521,  that 326\u001b[0m\n",
      "\n",
      "37: 10/10 (1.00), logits: 1.0000,  is 318\u001b[0m\n",
      "37: 49/50 (0.98), logits: 0.9935,  is 318\u001b[0m\n",
      "\n",
      "38: 9/10 (0.90), logits: 0.9657,  be 307\u001b[0m\n",
      "38: 41/50 (0.82), logits: 0.9278,  be 307\u001b[0m\n",
      "\n",
      "39: 10/10 (1.00), logits: 1.0000,  to 284\u001b[0m\n",
      "39: 46/50 (0.92), logits: 0.9938,  to 284\u001b[0m\n",
      "\n",
      "40: 10/10 (1.00), logits: 1.0000,  and 290\u001b[0m\n",
      "40: 50/50 (1.00), logits: 1.0000,  and 290\u001b[0m\n",
      "\n",
      "41: 10/10 (1.00), logits: 1.0000,  the 262\u001b[0m\n",
      "41: 46/50 (0.92), logits: 0.9768,  the 262\u001b[0m\n",
      "\n",
      "42: 10/10 (1.00), logits: 1.0000,  and 290\u001b[0m\n",
      "42: 50/50 (1.00), logits: 1.0000,  and 290\u001b[0m\n",
      "\n",
      "43: 10/10 (1.00), logits: 1.0000,  are 389\u001b[0m\n",
      "43: 45/50 (0.90), logits: 0.9628,  are 389\u001b[0m\n",
      "\n",
      "44: 10/10 (1.00), logits: 1.0000,  known 1900\u001b[0m\n",
      "44: 48/50 (0.96), logits: 0.9856,  known 1900\u001b[0m\n",
      "\n",
      "45: 10/10 (1.00), logits: 1.0000,  of 286\u001b[0m\n",
      "45: 48/50 (0.96), logits: 1.0000,  of 286\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positions = range(len(hidden))\n",
    "# positions = [6]\n",
    "k_options = [10, 50] # [1, 3, 5, 10, 50]\n",
    "\n",
    "for pos in positions:\n",
    "    last_layer_hidden = hidden[pos][-1].squeeze(0)[0,:]\n",
    "    last_layer_logits = logits[pos].squeeze(0)\n",
    "    position_topk_indices = torch.topk(last_layer_logits, k)[1]\n",
    "\n",
    "    exp_logits = torch.exp(last_layer_logits.to(torch.float64))\n",
    "\n",
    "    token_id = position_topk_indices[0]\n",
    "    token = tokenizer.decode(token_id)\n",
    "\n",
    "    out_logits, out_indices = out_emb_vector.forward(last_layer_hidden)\n",
    "    for j in k_options:\n",
    "        subset = position_topk_indices[0:j]\n",
    "        common_indices = subset[torch.isin(subset, out_indices)]\n",
    "        # print(common_indices.shape, exp_logits.shape, position_topk_indices.shape)\n",
    "        # print(common_indices)\n",
    "\n",
    "        exp_logits_all = exp_logits[position_topk_indices.squeeze(0)[0:j]]\n",
    "        exp_logits_common = exp_logits[common_indices]\n",
    "        logits_percentage = exp_logits_common.sum() / exp_logits_all.sum()\n",
    "        color = \"\\033[33m\" if logits_percentage < 0.9 else \"\"\n",
    "\n",
    "        print(f\"{color}{pos}: {len(common_indices)}/{j} ({len(common_indices)/j:0.2f}), logits: {logits_percentage:0.4f}, {token} {token_id}\\033[0m\")\n",
    "    if len(k_options) > 1: print()\n",
    "\n",
    "    if len(positions) == 1:\n",
    "        print(exp_logits_common / exp_logits_all.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvwQONhbY56D"
   },
   "outputs": [],
   "source": [
    "def double_multinomial(p1, p2):\n",
    "    assert len(p1.shape) == 1 and len(p2.shape) == 1\n",
    "\n",
    "    p1 = p1 / p1.sum()\n",
    "    p2 = p2 / p2.sum()\n",
    "\n",
    "    p1_cumsum = torch.cumsum(p1, dim=0)\n",
    "    p2_cumsum = torch.cumsum(p2, dim=0)\n",
    "\n",
    "    random_number = torch.rand(1).item()\n",
    "\n",
    "    i1 = torch.searchsorted(p1_cumsum, random_number).item()\n",
    "    i2 = torch.searchsorted(p2_cumsum, random_number).item()\n",
    "\n",
    "    return i1, i2, random_number\n",
    "\n",
    "out_emb_vector.index.set_ef(100)\n",
    "\n",
    "n_different_sample = 0\n",
    "\n",
    "num_return_sequences = 1\n",
    "max_length = 64\n",
    "tokens = tokenizer.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "xgen = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model_headless(xgen).last_hidden_state\n",
    "        last_hidden_state = last_hidden_state[:, -1, :]\n",
    "\n",
    "        logits, indices = out_emb_vector.forward(last_hidden_state)\n",
    "\n",
    "        logits_ref = last_hidden_state @ out_emb_weight.T\n",
    "        probs_ref = F.softmax(logits_ref, dim=-1)\n",
    "        topk_probs_ref, topk_indices_ref = torch.topk(probs_ref, 50, dim=-1)\n",
    "        # print(topk_probs_ref.shape, logits.shape)\n",
    "\n",
    "        # print(topk_probs_ref[0,0:10], topk_indices_ref[0,0:10])\n",
    "        # print(logits[0,0:10], indices[0,0:10])\n",
    "\n",
    "        # topk_probs_ref = F.softmax(topk_probs_ref.to(torch.float64), dim=-1)\n",
    "\n",
    "        exp_logits = F.softmax(logits.to(torch.float64), dim=-1)\n",
    "\n",
    "        xcol = []\n",
    "        for i in range(num_return_sequences):\n",
    "            i1, i2, ran = double_multinomial(exp_logits[i, :], topk_probs_ref[i, :])\n",
    "            i1 = torch.gather(indices[i,:], -1, torch.tensor(i1))\n",
    "            i2 = torch.gather(topk_indices_ref[i,:], -1, torch.tensor(i2))\n",
    "            xcol.append(i1)\n",
    "            if i1 != i2:\n",
    "\n",
    "                print(round(ran, 4), topk_probs_ref[i,0:10] / topk_probs_ref[i,0:10].sum())\n",
    "                print(round(ran, 4), exp_logits[i,0:10] / exp_logits[i,0:10].sum())\n",
    "\n",
    "                print(tokenizer.decode(xgen[i, -32:max_length].tolist()), f\"'{tokenizer.decode(i1.tolist())}'/'{tokenizer.decode(i2.tolist())}'\")\n",
    "                n_different_sample += 1\n",
    "\n",
    "        xcol = torch.tensor(xcol).unsqueeze(0)\n",
    "        # ix = torch.multinomial(exp_logits, 1)\n",
    "        # xcol = torch.gather(indices, -1, ix)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "print()\n",
    "print(\"Results:\")\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    print(tokenizer.decode(tokens))\n",
    "\n",
    "print(f\"{n_different_sample/(max_length * num_return_sequences):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtBnMIBnvcvI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPE38s751rPLCoSA+fQ96od",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
