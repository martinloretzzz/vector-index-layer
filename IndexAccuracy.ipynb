{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch.nn as nn\n",
    "import hnswlib\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model_ref = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNSWIndexEmbedding():\n",
    "    def __init__(self, weight, k, M=32, ef=100, ef_construction=100, index_file=None):\n",
    "        self.k, self.vocab_size, self.dim = k, weight.shape[0], weight.shape[1]\n",
    "        self.index = hnswlib.Index(space='ip', dim=self.dim)\n",
    "        self.index.init_index(max_elements=self.vocab_size, M=M, ef_construction=ef_construction, random_seed=42)\n",
    "\n",
    "        index_path = f\"{index_file}-{M}-{ef_construction}.index\"\n",
    "        if index_file is None or not os.path.exists(index_path):\n",
    "            self.index.add_items(weight.numpy())\n",
    "            if index_file is not None:\n",
    "                self.index.save_index(index_path)\n",
    "        else:\n",
    "            print(f\"Loading index from file: {index_path}\")\n",
    "            self.index.load_index(index_path)\n",
    "        self.index.set_ef(ef)\n",
    "\n",
    "    def forward(self, x):\n",
    "        indices, distances = self.index.knn_query(x.detach().cpu().numpy(), k=self.k)\n",
    "        return torch.from_numpy(1 - distances).to(torch.float32).to(x.device), torch.from_numpy(indices).to(torch.int64).to(x.device)\n",
    "\n",
    "\n",
    "class HNSWLogitsEmbedding(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(-1, x.shape[-1])\n",
    "        distances, indices = self.layer.forward(x_flat)\n",
    "   \n",
    "        logits = torch.full((x_flat.shape[0], self.layer.vocab_size), float(\"-inf\"), dtype=torch.float32, device=x.device)\n",
    "        logits.scatter_(-1, indices, distances)\n",
    "        return logits.view((x.shape[0], x.shape[1], self.layer.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = model.lm_head.weight.detach().clone()\n",
    "model.lm_head = HNSWLogitsEmbedding(HNSWIndexEmbedding(weight, k=50, ef=200, M=32, ef_construction=1000, index_file=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12122 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "test = test.select(range(200))\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_list(tensor, batch_size):\n",
    "    return [tensor[:, i:i+batch_size] for i in range(0, tensor.size(1), batch_size)]\n",
    "\n",
    "batch_size = 128\n",
    "input_ids_list = batch_list(encodings.input_ids, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.9909, Lower 75%: 1/128\n",
      "Ratio: 0.9876, Lower 75%: 0/128\n",
      "Ratio: 0.9918, Lower 75%: 1/128\n",
      "Ratio: 0.9916, Lower 75%: 0/128\n",
      "Ratio: 0.9806, Lower 75%: 3/128\n",
      "Ratio: 0.9859, Lower 75%: 2/128\n",
      "Ratio: 0.9932, Lower 75%: 0/128\n",
      "Ratio: 0.9924, Lower 75%: 1/128\n",
      "Ratio: 0.9884, Lower 75%: 2/128\n",
      "Ratio: 0.9966, Lower 75%: 0/128\n",
      "Ratio: 0.9963, Lower 75%: 0/128\n",
      "Ratio: 0.9829, Lower 75%: 4/128\n",
      "Ratio: 0.9941, Lower 75%: 0/128\n",
      "Ratio: 0.9964, Lower 75%: 0/128\n",
      "Ratio: 0.9978, Lower 75%: 0/128\n",
      "Ratio: 0.9973, Lower 75%: 0/128\n",
      "Ratio: 0.9985, Lower 75%: 0/128\n",
      "Ratio: 0.9944, Lower 75%: 0/128\n",
      "Ratio: 0.9988, Lower 75%: 0/128\n",
      "Ratio: 0.9895, Lower 75%: 2/128\n",
      "Ratio: 0.9967, Lower 75%: 0/128\n",
      "Ratio: 0.9980, Lower 75%: 0/128\n",
      "Ratio: 0.9974, Lower 75%: 0/128\n",
      "Ratio: 0.9948, Lower 75%: 1/128\n",
      "Ratio: 0.9981, Lower 75%: 0/128\n",
      "Ratio: 0.9987, Lower 75%: 0/128\n",
      "Ratio: 0.9960, Lower 75%: 0/128\n",
      "Ratio: 0.9964, Lower 75%: 0/128\n",
      "Ratio: 0.9969, Lower 75%: 0/128\n",
      "Ratio: 0.9958, Lower 75%: 0/128\n",
      "Ratio: 0.9980, Lower 75%: 0/128\n",
      "Ratio: 0.9935, Lower 75%: 1/128\n",
      "Ratio: 0.9924, Lower 75%: 0/128\n",
      "Ratio: 0.9875, Lower 75%: 2/128\n",
      "Ratio: 0.9843, Lower 75%: 2/128\n",
      "Ratio: 0.9932, Lower 75%: 0/128\n",
      "Ratio: 0.9963, Lower 75%: 0/128\n",
      "Ratio: 0.9947, Lower 75%: 0/128\n",
      "Ratio: 0.9844, Lower 75%: 1/128\n",
      "Ratio: 0.9966, Lower 75%: 0/128\n",
      "Ratio: 0.9925, Lower 75%: 0/128\n",
      "Ratio: 0.9941, Lower 75%: 0/128\n",
      "Ratio: 0.9968, Lower 75%: 0/128\n",
      "Ratio: 0.9958, Lower 75%: 0/128\n",
      "Ratio: 0.9965, Lower 75%: 0/128\n",
      "Ratio: 0.9948, Lower 75%: 0/128\n",
      "Ratio: 0.9975, Lower 75%: 0/128\n",
      "Ratio: 0.9931, Lower 75%: 2/128\n",
      "Ratio: 0.9964, Lower 75%: 0/128\n",
      "Ratio: 0.9974, Lower 75%: 0/128\n",
      "Ratio: 0.9894, Lower 75%: 2/128\n",
      "Ratio: 0.9907, Lower 75%: 1/128\n",
      "Ratio: 0.9896, Lower 75%: 1/128\n",
      "Ratio: 0.9831, Lower 75%: 3/128\n",
      "Ratio: 0.9948, Lower 75%: 0/128\n",
      "Ratio: 0.9880, Lower 75%: 1/128\n",
      "Ratio: 0.9893, Lower 75%: 2/128\n",
      "Ratio: 0.9842, Lower 75%: 1/128\n",
      "Ratio: 0.9893, Lower 75%: 1/128\n",
      "Ratio: 0.9966, Lower 75%: 0/128\n",
      "Ratio: 0.9941, Lower 75%: 0/128\n",
      "Ratio: 0.9950, Lower 75%: 0/128\n",
      "Ratio: 0.9912, Lower 75%: 1/128\n",
      "Ratio: 0.9943, Lower 75%: 0/128\n",
      "Ratio: 0.9969, Lower 75%: 0/128\n",
      "Ratio: 0.9983, Lower 75%: 0/128\n",
      "Ratio: 0.9944, Lower 75%: 1/128\n",
      "Ratio: 0.9943, Lower 75%: 0/128\n",
      "Ratio: 0.9954, Lower 75%: 1/128\n",
      "Ratio: 0.9950, Lower 75%: 0/128\n",
      "Ratio: 0.9895, Lower 75%: 2/128\n",
      "Ratio: 0.9883, Lower 75%: 1/128\n",
      "Ratio: 0.9796, Lower 75%: 2/128\n",
      "Ratio: 0.9875, Lower 75%: 1/128\n",
      "Ratio: 0.9929, Lower 75%: 0/128\n",
      "Ratio: 0.9898, Lower 75%: 2/128\n",
      "Ratio: 0.9970, Lower 75%: 0/128\n",
      "Ratio: 0.9919, Lower 75%: 0/128\n",
      "Ratio: 0.9949, Lower 75%: 0/128\n",
      "Ratio: 0.9929, Lower 75%: 0/128\n",
      "Ratio: 0.9971, Lower 75%: 0/128\n",
      "Ratio: 0.9948, Lower 75%: 0/128\n",
      "Ratio: 0.9883, Lower 75%: 1/128\n",
      "Ratio: 0.9975, Lower 75%: 0/128\n",
      "Ratio: 0.9929, Lower 75%: 1/128\n",
      "Ratio: 0.9964, Lower 75%: 0/128\n",
      "Ratio: 0.9971, Lower 75%: 0/128\n",
      "Ratio: 0.9979, Lower 75%: 0/128\n",
      "Ratio: 0.9888, Lower 75%: 2/128\n",
      "Ratio: 0.9943, Lower 75%: 0/128\n",
      "Ratio: 0.9922, Lower 75%: 1/128\n",
      "Ratio: 0.9987, Lower 75%: 0/128\n",
      "Ratio: 0.9960, Lower 75%: 0/128\n",
      "Ratio: 0.9961, Lower 75%: 0/128\n",
      "Ratio: 0.9984, Lower 75%: 0/90\n",
      "Average ratio: 0.9933, Average lower 75: 0.0044\n"
     ]
    }
   ],
   "source": [
    "ratio_accum, error_accum = 0, 0\n",
    "\n",
    "for input_ids in input_ids_list:\n",
    "    # print(input_ids.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get logits from model and model_ref\n",
    "        outputs = model(input_ids)\n",
    "        outputs_ref = model_ref(input_ids)\n",
    "\n",
    "        logits = outputs.logits.squeeze(0).to(torch.float64)\n",
    "        logits_ref = outputs_ref.logits.squeeze(0).to(torch.float64)\n",
    "\n",
    "        topk_indices = torch.topk(logits, k=50, dim=-1)[1]\n",
    "        topk_indices_ref = torch.topk(logits_ref, k=50, dim=-1)[1]\n",
    "\n",
    "        # topk_indices = topk_indices_ref\n",
    "\n",
    "        # gather logits from topk_indices\n",
    "        # print(torch.gather(logits, -1, topk_indices))\n",
    "        exp = torch.exp(torch.gather(logits, -1, topk_indices))\n",
    "        exp_ref = torch.exp(torch.gather(logits_ref, -1, topk_indices_ref))\n",
    "\n",
    "        # replace nan with 0\n",
    "        #small_value = 1e-32\n",
    "        #exp[torch.isnan(exp)] = small_value\n",
    "        #exp_ref[torch.isnan(exp_ref)] = small_value\n",
    "\n",
    "        \n",
    "        # print(exp.sum(-1).shape)\n",
    "        ratios = exp.sum(-1) / exp_ref.sum(-1)\n",
    "        lower_80 = (ratios < 0.75).sum() \n",
    "        # print(ratios)\n",
    "        ratio = ratios.mean()\n",
    "        # print(exp.sum(-1) / exp_ref.sum(-1))\n",
    "        ratio_accum += ratio\n",
    "        error_accum += lower_80 / ratios.numel()\n",
    "\n",
    "        print(f\"Ratio: {ratio.item():.4f}, Lower 75%: {lower_80.item()}/{ratios.numel()}\") \n",
    "\n",
    "print(f\"Average ratio: {ratio_accum / len(input_ids_list):.4f}, Average lower 75: {error_accum / len(input_ids_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.layer.index.ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M=40 ef=150 err=1.8%\n",
    "# M=32 ef=150 err=1.64%\n",
    "# M=48 ef=150 err=1.47%\n",
    "# sample both topk and compare ratio\n",
    "# M=32 ef=150 err=1.39%\n",
    "# M=64 ef=200 err=0.64\n",
    "# M=64 ef=150 err=0.97\n",
    "# M=48 ef=200 err=0.83\n",
    "# M=32 ef=200 err=0.92\n",
    "# M=32 ef=200 err=0.61 ef_construction=300\n",
    "# M=32 ef=200 err=0.44 ef_construction=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
