{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinloretzzz/vector-index-layer/blob/main/VectorIndexLayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hnswlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsNCstCKwMqG",
        "outputId": "41aa8a40-4af5-44ab-ede4-2c754a18566c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hnswlib in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hnswlib) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m9Fv-bjlx8Ot"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Model\n",
        "import hnswlib\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import timeit\n",
        "import time\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a60r2gddzutA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cca62fe-e07d-4300-cbb6-78a4d1d0c610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox-like movements of his arm, which look just like those of a bull and a bear, will make me look like a tiger. But look what he has for me!\"\n",
            "\n",
            "Bobby's voice had become muffled.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model_headless = GPT2Model.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "input_text = \"The quick brown fox\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    return_dict_in_generate=True,\n",
        "    output_hidden_states=True,\n",
        "    output_scores=True,\n",
        "    output_logits=True\n",
        ")\n",
        "\n",
        "gen_tokens = output.sequences\n",
        "\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "print(gen_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lh9EoUL5Dul",
        "outputId": "e8b3ae43-c736-43af-bfdb-a18ff7440bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46 46 13\n",
            "torch.Size([1, 768])\n",
            "torch.Size([1, 50257])\n"
          ]
        }
      ],
      "source": [
        "hidden = output.hidden_states\n",
        "logits = output.logits\n",
        "\n",
        "last_hidden = hidden[-1][-1].squeeze(0)\n",
        "last_logits = logits[-1]\n",
        "\n",
        "print(len(hidden), len(logits), len(hidden[-1]))\n",
        "print(last_hidden.shape)\n",
        "print(last_logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HNSW Index Embedding Layer"
      ],
      "metadata": {
        "id": "y-SOGu-utf3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HNSWIndexEmbedding():\n",
        "    def __init__(self, weight, k, M=32, ef=100, ef_construction=100):\n",
        "        self.k, self.vocab_size, self.dim = k, weight.shape[0], weight.shape[1]\n",
        "        self.index = hnswlib.Index(space='ip', dim=self.dim)\n",
        "        self.index.init_index(max_elements=self.vocab_size, M=M, ef_construction=ef_construction, random_seed=42)\n",
        "        self.index.add_items(weight.numpy())\n",
        "        self.index.set_ef(ef)\n",
        "\n",
        "    def forward(self, x):\n",
        "        indices, distances = self.index.knn_query(x.detach().cpu().numpy(), k=self.k)\n",
        "        return torch.from_numpy(1 - distances).to(torch.float32).to(x.device), torch.from_numpy(indices).to(torch.int64).to(x.device)"
      ],
      "metadata": {
        "id": "F5xvNoOPxPmJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 50\n",
        "\n",
        "out_emb_weight = model.transformer.wte.weight.detach().clone()\n",
        "print(out_emb_weight.shape)\n",
        "\n",
        "out_emb_vector = HNSWIndexEmbedding(out_emb_weight, k=k, ef_construction=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYOubClf4eBb",
        "outputId": "e3dc05eb-7735-440b-b09b-452214991cbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_logits, out_indices = out_emb_vector.forward(last_hidden)\n",
        "out_logits, out_indices"
      ],
      "metadata": {
        "id": "OrjPOoSq5xJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64621264-bd26-40cc-ca62-70a5c6644c04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-106.2140, -106.8267, -106.8412, -107.0850, -107.6768, -108.0547,\n",
              "          -108.6532, -108.7733, -108.8456, -109.0358, -109.3222, -109.5707,\n",
              "          -109.6564, -109.9158, -109.9889, -110.0014, -110.0058, -110.0616,\n",
              "          -110.0690, -110.1507, -110.1909, -110.2026, -110.2058, -110.2102,\n",
              "          -110.3968, -110.4023, -110.4185, -110.4547, -110.6066, -110.7092,\n",
              "          -110.7136, -110.7649, -110.9780, -111.0564, -111.1580, -111.1598,\n",
              "          -111.2218, -111.2827, -111.2983, -111.3177, -111.3209, -111.4116,\n",
              "          -111.4208, -111.4266, -111.5473, -111.6009, -111.6026, -111.6071,\n",
              "          -111.6134, -111.6419]]),\n",
              " tensor([[  416,    13,    11,   355,   290,   287,   351,   422,   329,   618,\n",
              "            772,   379,   783,   832,   284,   739,    26,   757,   257,   319,\n",
              "            625,   780,  2157,   475,   878,  1752,   523,   588,   706,    25,\n",
              "            477,  2029,  3690,  2048,   656,   981,   517,   655,  1165,   262,\n",
              "          11061,  1626,   503,  1201,  1865,   691,  1871,  1566,  6451,  1088]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate & Prediction Similarity Measurement"
      ],
      "metadata": {
        "id": "TdyIebB8uFjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positions = range(len(hidden))\n",
        "# positions = [6]\n",
        "k_options = [50] # [1, 3, 5, 10, 50]\n",
        "\n",
        "for pos in positions:\n",
        "    last_layer_hidden = hidden[pos][-1].squeeze(0)[0,:]\n",
        "    last_layer_logits = logits[pos].squeeze(0)\n",
        "    position_topk_indices = torch.topk(last_layer_logits, k)[1]\n",
        "\n",
        "    exp_logits = torch.exp(last_layer_logits.to(torch.float64))\n",
        "\n",
        "    token_id = position_topk_indices[0]\n",
        "    token = tokenizer.decode(token_id)\n",
        "\n",
        "    out_logits, out_indices = out_emb_vector.forward(last_layer_hidden)\n",
        "    for j in k_options:\n",
        "        subset = position_topk_indices[0:j]\n",
        "        common_indices = subset[torch.isin(subset, out_indices)]\n",
        "\n",
        "        exp_logits_all = exp_logits[position_topk_indices.squeeze(0)[0:j]]\n",
        "        exp_logits_common = exp_logits[common_indices]\n",
        "        logits_percentage = exp_logits_common.sum() / exp_logits_all.sum()\n",
        "        color = \"\\033[33m\" if logits_percentage < 0.9 else \"\"\n",
        "\n",
        "        print(f\"{color}{pos}: {len(common_indices)}/{j} ({len(common_indices)/j:0.2f}), logits: {logits_percentage:0.4f}, {token} {token_id}\\033[0m\")\n",
        "    if len(k_options) > 1: print()"
      ],
      "metadata": {
        "id": "ECv9TITu701E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641d55e8-cd25-4a78-c967-08002cf14033"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m0: 15/50 (0.30), logits: 0.4504, es 274\u001b[0m\n",
            "1: 48/50 (0.96), logits: 0.9810, like 2339\u001b[0m\n",
            "2: 44/50 (0.88), logits: 0.9229,  creature 7185\u001b[0m\n",
            "3: 49/50 (0.98), logits: 0.9909,  of 286\u001b[0m\n",
            "4: 50/50 (1.00), logits: 1.0000,  the 262\u001b[0m\n",
            "\u001b[33m5: 40/50 (0.80), logits: 0.8770,  tail 7894\u001b[0m\n",
            "6: 48/50 (0.96), logits: 0.9853,  made 925\u001b[0m\n",
            "7: 50/50 (1.00), logits: 1.0000,  which 543\u001b[0m\n",
            "8: 48/50 (0.96), logits: 0.9912,  he 339\u001b[0m\n",
            "9: 47/50 (0.94), logits: 0.9799,  like 588\u001b[0m\n",
            "10: 44/50 (0.88), logits: 0.9970,  like 588\u001b[0m\n",
            "11: 47/50 (0.94), logits: 0.9951,  a 257\u001b[0m\n",
            "12: 46/50 (0.92), logits: 0.9951,  of 286\u001b[0m\n",
            "13: 49/50 (0.98), logits: 0.9990,  a 257\u001b[0m\n",
            "14: 44/50 (0.88), logits: 0.9522,  fox 21831\u001b[0m\n",
            "15: 40/50 (0.80), logits: 0.9763, dog 9703\u001b[0m\n",
            "\u001b[33m16: 48/50 (0.96), logits: 0.8570,  a 257\u001b[0m\n",
            "\u001b[33m17: 43/50 (0.86), logits: 0.8090,  dog 3290\u001b[0m\n",
            "18: 46/50 (0.92), logits: 0.9917, , 11\u001b[0m\n",
            "19: 48/50 (0.96), logits: 0.9889,  are 389\u001b[0m\n",
            "20: 46/50 (0.92), logits: 0.9715,  make 787\u001b[0m\n",
            "21: 49/50 (0.98), logits: 0.9991,  you 345\u001b[0m\n",
            "22: 46/50 (0.92), logits: 0.9767,  think 892\u001b[0m\n",
            "23: 48/50 (0.96), logits: 0.9946,  like 588\u001b[0m\n",
            "24: 50/50 (1.00), logits: 1.0000,  a 257\u001b[0m\n",
            "\u001b[33m25: 41/50 (0.82), logits: 0.7593,  bull 6473\u001b[0m\n",
            "26: 50/50 (1.00), logits: 1.0000, . 13\u001b[0m\n",
            "27: 50/50 (1.00), logits: 1.0000, \n",
            " 198\u001b[0m\n",
            "28: 49/50 (0.98), logits: 0.9970,  I 314\u001b[0m\n",
            "29: 50/50 (1.00), logits: 1.0000,  at 379\u001b[0m\n",
            "30: 48/50 (0.96), logits: 0.9976,  I 314\u001b[0m\n",
            "31: 50/50 (1.00), logits: 1.0000,  does 857\u001b[0m\n",
            "32: 48/50 (0.96), logits: 0.9926,  done 1760\u001b[0m\n",
            "33: 45/50 (0.90), logits: 0.9909,  me 502\u001b[0m\n",
            "34: 50/50 (1.00), logits: 1.0000, . 13\u001b[0m\n",
            "35: 47/50 (0.94), logits: 0.9931, \n",
            " 198\u001b[0m\n",
            "36: 47/50 (0.94), logits: 0.9992, \n",
            " 198\u001b[0m\n",
            "37: 48/50 (0.96), logits: 0.9898, \" 1\u001b[0m\n",
            "38: 44/50 (0.88), logits: 0.9018, ai 1872\u001b[0m\n",
            "39: 49/50 (0.98), logits: 0.9924, 's 338\u001b[0m\n",
            "40: 49/50 (0.98), logits: 0.9962,  eyes 2951\u001b[0m\n",
            "41: 42/50 (0.84), logits: 0.9182,  was 373\u001b[0m\n",
            "42: 46/50 (0.92), logits: 0.9652,  a 257\u001b[0m\n",
            "43: 42/50 (0.84), logits: 0.9319,  a 257\u001b[0m\n",
            "44: 48/50 (0.96), logits: 1.0000, led 992\u001b[0m\n",
            "45: 48/50 (0.96), logits: 0.9943,  by 416\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples simultaneously from the hnsw and the reference(full matrix multiplication) distribution\n",
        "# and logs all the positions where a different token are smapled.\n",
        "\n",
        "def double_multinomial(p1, p2):\n",
        "    assert len(p1.shape) == 1 and len(p2.shape) == 1\n",
        "\n",
        "    p1 = p1 / p1.sum()\n",
        "    p2 = p2 / p2.sum()\n",
        "\n",
        "    p1_cumsum = torch.cumsum(p1, dim=0)\n",
        "    p2_cumsum = torch.cumsum(p2, dim=0)\n",
        "\n",
        "    random_number = torch.rand(1).item()\n",
        "\n",
        "    i1 = torch.searchsorted(p1_cumsum, random_number).item()\n",
        "    i2 = torch.searchsorted(p2_cumsum, random_number).item()\n",
        "\n",
        "    return i1, i2, random_number\n",
        "\n",
        "out_emb_vector.index.set_ef(100)\n",
        "\n",
        "n_different_sample = 0\n",
        "max_length = 64\n",
        "tokens = tokenizer.encode(\"Hello, I'm a language model,\")\n",
        "xgen = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "while xgen.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        last_hidden_state = model_headless(xgen).last_hidden_state\n",
        "        last_hidden_state = last_hidden_state[:, -1, :]\n",
        "\n",
        "        logits_vec, indices_vec = out_emb_vector.forward(last_hidden_state)\n",
        "\n",
        "        logits_ref = last_hidden_state @ out_emb_weight.T\n",
        "        probs_ref = F.softmax(logits_ref, dim=-1)\n",
        "        topk_probs_ref, topk_indices_ref = torch.topk(probs_ref, 50, dim=-1)\n",
        "\n",
        "        exp_logits = F.softmax(logits_vec.to(torch.float64), dim=-1)\n",
        "\n",
        "        i1, i2, ran = double_multinomial(exp_logits[0, :], topk_probs_ref[0, :])\n",
        "        i1 = torch.gather(indices_vec[0,:], -1, torch.tensor(i1))\n",
        "        i2 = torch.gather(topk_indices_ref[0,:], -1, torch.tensor(i2))\n",
        "        xcol = i1.view(1, 1)\n",
        "\n",
        "        if i1 != i2:\n",
        "            print(tokenizer.decode(xgen[0, -32:max_length].tolist()), f\"'{tokenizer.decode(i1.tolist())}'/'{tokenizer.decode(i2.tolist())}'\")\n",
        "            n_different_sample += 1\n",
        "\n",
        "        xgen = torch.cat((xgen, xcol), dim=1)\n",
        "\n",
        "print(\"\\n\\nGenerated Text:\")\n",
        "tokens = xgen[0, :max_length].tolist()\n",
        "print(tokenizer.decode(tokens))\n",
        "\n",
        "print(f\"{n_different_sample/max_length:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvwQONhbY56D",
        "outputId": "15e06100-94d4-4123-e09a-f4966b48c3e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I'm a language model, and this is the most important one for any ' programming'/' of'\n",
            "Hello, I'm a language model, and this is the most important one for any programming language '.'/','\n",
            "Hello, I'm a language model, and this is the most important one for any programming language.\n",
            "\n",
            " 'No'/'Hello'\n",
            "Hello, I'm a language model, and this is the most important one for any programming language.\n",
            "\n",
            "No I am not an ' editor'/' engineer'\n",
            ", I'm a language model, and this is the most important one for any programming language.\n",
            "\n",
            "No I am not an editor and I do not write ' this'/' languages'\n",
            " the most important one for any programming language.\n",
            "\n",
            "No I am not an editor and I do not write this code. I should only write this code so ' it'/' others'\n",
            " for any programming language.\n",
            "\n",
            "No I am not an editor and I do not write this code. I should only write this code so it will have a ' meaning'/' higher'\n",
            " I am not an editor and I do not write this code. I should only write this code so it will have a meaning that people can understand.\n",
            "\n",
            " 'And'/'There'\n",
            " editor and I do not write this code. I should only write this code so it will have a meaning that people can understand.\n",
            "\n",
            "And then I start ' writing'/' coding'\n",
            "\n",
            "\n",
            "Generated Text:\n",
            "Hello, I'm a language model, and this is the most important one for any programming language.\n",
            "\n",
            "No I am not an editor and I do not write this code. I should only write this code so it will have a meaning that people can understand.\n",
            "\n",
            "And then I start writing the program using code\n",
            "0.1406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate & Performance Measurement"
      ],
      "metadata": {
        "id": "_nO9k3SStm3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_emb_vector.index.set_ef(100)\n",
        "\n",
        "forward_time = timeit.timeit(lambda: out_emb_vector.forward(last_hidden), number=10)\n",
        "forward_ref_time = timeit.timeit(lambda: last_hidden @ out_emb_weight.T, number=10)\n",
        "\n",
        "print(f\"Average time taken (forward): {forward_time:.6f} seconds\")\n",
        "print(f\"Average time taken (matrix multiplication): {forward_ref_time:.6f} seconds\")\n",
        "print(f\"Speedup: {forward_ref_time / forward_time:.4f}\")"
      ],
      "metadata": {
        "id": "MubsgL5kLWsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31234886-0442-4e1c-8abf-12af66d2d413"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average time taken (forward): 0.006952 seconds\n",
            "Average time taken (matrix multiplication): 0.160113 seconds\n",
            "Speedup: 23.0304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using top-k sampling from a GPT-2 model without the LM head,\n",
        "# utilizing a vector index to get the top-k elements (or without the index if method=ref)\n",
        "def generate(method=\"vec-index\", num_return_sequences=4, max_length=64):\n",
        "    tokens = tokenizer.encode(\"Hello, I'm a language model,\")\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    xgen = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "    while xgen.size(1) < max_length:\n",
        "        with torch.no_grad():\n",
        "            last_hidden_state = model_headless(xgen).last_hidden_state\n",
        "            last_hidden_state = last_hidden_state[:, -1, :]\n",
        "\n",
        "            if method == \"vec-index\":\n",
        "                logits, indices = out_emb_vector.forward(last_hidden_state)\n",
        "                exp_logits = F.softmax(logits.to(torch.float64), dim=-1)\n",
        "            else:\n",
        "                logits_ref = last_hidden_state @ out_emb_weight.T\n",
        "                probs_ref = F.softmax(logits_ref, dim=-1)\n",
        "                exp_logits, indices = torch.topk(probs_ref, 50, dim=-1)\n",
        "\n",
        "            ix = torch.multinomial(exp_logits, 1)\n",
        "            xcol = torch.gather(indices, -1, ix)\n",
        "            xgen = torch.cat((xgen, xcol), dim=1)\n",
        "    return xgen\n",
        "\n",
        "out_emb_vector.index.set_ef(100)\n",
        "\n",
        "start = time.time()\n",
        "xgen = generate(\"vec-index\", max_length=32, num_return_sequences=4)\n",
        "vec_time = time.time() - start\n",
        "print(f\"Vec took {vec_time:.2f} seconds\")\n",
        "\n",
        "start = time.time()\n",
        "xgen = generate(\"ref\", max_length=32, num_return_sequences=4)\n",
        "ref_time = time.time() - start\n",
        "print(f\"Ref took {ref_time:.2f} seconds\")\n",
        "\n",
        "print(f\"Speedup: {ref_time / vec_time:.2f}\")\n",
        "\n",
        "# for i in range(num_return_sequences):\n",
        "#    print(tokenizer.decode(xgen[i, :max_length].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOwOJbOFSqWn",
        "outputId": "ddc31050-7d00-4d81-e2f8-bddc32d1ce82"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vec took 7.65 seconds\n",
            "Ref took 9.55 seconds\n",
            "Speedup: 1.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    data = torch.cat([hidden[i][-1].squeeze(0) for i in range(len(hidden))], dim=0).repeat(6, 1)\n",
        "    time_repeat, time_num = 10, 10\n",
        "\n",
        "    print(\"| B   | ef  | Speedup |\")\n",
        "    print(\"| --: | --: | ------: |\")\n",
        "    for ef in [100, 200]:\n",
        "      for B in [1, 8, 54, 256]:\n",
        "        out_emb_vector.index.set_ef(ef)\n",
        "        batch = data[0:B, :]\n",
        "\n",
        "        forward_time = min(timeit.repeat(lambda: out_emb_vector.forward(batch), number=time_num, repeat=time_repeat))\n",
        "        forward_ref_time = min(timeit.repeat(lambda: batch @ out_emb_weight.T, number=time_num, repeat=time_repeat))\n",
        "\n",
        "        print(f\"|  {B}  | {ef} | {forward_ref_time / forward_time:.1f}x |\")"
      ],
      "metadata": {
        "id": "oufU6Ml-B95W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NtBnMIBnvcvI"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjySEXVfG9e/GplO0oXf8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}